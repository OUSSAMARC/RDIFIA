{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgWb11WT-07r"
      },
      "source": [
        "# RDFIA: Practical work 1-cd\n",
        "## Convolutional Neural Networks\n",
        "### Sorbonne Universit√© 2024\n",
        "### [Course link](https://rdfia.github.io/)\n",
        "### GRUSS Carlos, RCHAKI Oussama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!git clone https://github.com/cdancette/deep-learning-polytech-tp6-7.git\n",
        "\n",
        "# Only run if \"utils.py\" is not already downloaded\n",
        "if not os.path.exists(\"utils.py\"):\n",
        "    # Download the file\n",
        "    ! wget https://github.com/rdfia/rdfia.github.io/raw/master/code/2-cd/utils.py\n",
        "\n",
        "%run \"utils.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8I7lgAEJvPCh"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import v2\n",
        "import torchvision.datasets as datasets\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.decomposition import PCA\n",
        "from torchvision.transforms import Lambda\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from utils import *\n",
        "\n",
        "PRINT_INTERVAL = 200\n",
        "PATH=\"datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3xVkUxvwy6a0"
      },
      "outputs": [],
      "source": [
        "def pca_whitening(image):\n",
        "    \"\"\"\n",
        "    Apply PCA whitening to an image. Assumes the input is a 3D numpy array (H x W x C).\n",
        "    \"\"\"\n",
        "    flat_img = image.reshape(-1, 3)\n",
        "    # Apply PCA\n",
        "    pca = PCA(whiten=True)\n",
        "    whitened = pca.fit_transform(flat_img)\n",
        "    # Reshape to original dimensions\n",
        "    whitened_img = whitened.reshape(image.shape)\n",
        "    return torch.from_numpy(whitened_img).float()\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the structure of the neural network\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # We first define the convolution and pooling layers as a features extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, (5, 5), stride=1, padding=2),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(6, 16, (5, 5), stride=1, padding=0),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "        )\n",
        "        # We then define fully connected layers as a classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(400, 120),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(84, 10)\n",
        "            # Reminder: The softmax is included in the loss, do not put it here\n",
        "        )\n",
        "\n",
        "    # Method called when we apply the network to an input batch\n",
        "    def forward(self, input):\n",
        "        bsize = input.size(0) # batch size\n",
        "        output = self.features(input) # output of the conv layers\n",
        "        output = output.view(bsize, -1) # we flatten the 2D feature maps into one 1D vector for each input\n",
        "        output = self.classifier(output) # we compute the output of the fc layers\n",
        "        return output\n",
        "\n",
        "class ConvNet_v2(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the structure of the neural network\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(ConvNet_v2, self).__init__()\n",
        "        # We first define the convolution and pooling layers as a features extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0, ceil_mode=True)\n",
        "        )\n",
        "        # We then define fully connected layers as a classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=1024, out_features=1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 10)\n",
        "            # Reminder: The softmax is included in the loss, do not put it here\n",
        "        )\n",
        "\n",
        "    # Method called when we apply the network to an input batch\n",
        "    def forward(self, input):\n",
        "        bsize = input.size(0) # batch size\n",
        "        output = self.features(input) # output of the conv layers\n",
        "        output = output.view(bsize, -1) # we flatten the 2D feature maps into one 1D vector for each input\n",
        "        output = self.classifier(output) # we compute the output of the fc layers\n",
        "        return output\n",
        "\n",
        "class ConvNet_v3(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the structure of the neural network\n",
        "    \"\"\"\n",
        "    def __init__(self, prob=0.5):\n",
        "        super(ConvNet_v3, self).__init__()\n",
        "        # We first define the convolution and pooling layers as a features extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0, ceil_mode=True)\n",
        "        )\n",
        "        # We then define fully connected layers as a classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=1024, out_features=1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=prob),\n",
        "            nn.Linear(1000, 10)\n",
        "            # Reminder: The softmax is included in the loss, do not put it here\n",
        "        )\n",
        "\n",
        "    # Method called when we apply the network to an input batch\n",
        "    def forward(self, input):\n",
        "        bsize = input.size(0) # batch size\n",
        "        output = self.features(input) # output of the conv layers\n",
        "        output = output.view(bsize, -1) # we flatten the 2D feature maps into one 1D vector for each input\n",
        "        output = self.classifier(output) # we compute the output of the fc layers\n",
        "        return output\n",
        "    \n",
        "class ConvNet_v4(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the structure of the neural network\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(ConvNet_v4, self).__init__()\n",
        "        # We first define the convolution and pooling layers as a features extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(5, 5), stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=1, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5, 5), stride=1, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0, ceil_mode=True)\n",
        "        )\n",
        "        # We then define fully connected layers as a classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=1024, out_features=1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 10)\n",
        "            # Reminder: The softmax is included in the loss, do not put it here\n",
        "        )\n",
        "\n",
        "    # Method called when we apply the network to an input batch\n",
        "    def forward(self, input):\n",
        "        bsize = input.size(0) # batch size\n",
        "        output = self.features(input) # output of the conv layers\n",
        "        output = output.view(bsize, -1) # we flatten the 2D feature maps into one 1D vector for each input\n",
        "        output = self.classifier(output) # we compute the output of the fc layers\n",
        "        return output\n",
        "\n",
        "class ConvNet_v5(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the structure of the neural network\n",
        "    \"\"\"\n",
        "    def __init__(self, prob=0.5):\n",
        "        super(ConvNet_v5, self).__init__()\n",
        "        # We first define the convolution and pooling layers as a features extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(5, 5), stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=1, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5, 5), stride=1, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0, ceil_mode=True)\n",
        "        )\n",
        "        # We then define fully connected layers as a classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=1024, out_features=1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=prob),\n",
        "            nn.Linear(1000, 10)\n",
        "            # Reminder: The softmax is included in the loss, do not put it here\n",
        "        )\n",
        "\n",
        "    # Method called when we apply the network to an input batch\n",
        "    def forward(self, input):\n",
        "        bsize = input.size(0) # batch size\n",
        "        output = self.features(input) # output of the conv layers\n",
        "        output = output.view(bsize, -1) # we flatten the 2D feature maps into one 1D vector for each input\n",
        "        output = self.classifier(output) # we compute the output of the fc layers\n",
        "        return output\n",
        "\n",
        "def get_dataset(batch_size, cuda=False, task=\"mnist\", transform=None, augment=None):\n",
        "    \"\"\"\n",
        "    This function loads the dataset and performs transformations on each\n",
        "    image (listed in `transform = ...`).\n",
        "    \"\"\"\n",
        "    if task == \"mnist\": \n",
        "        train_dataset = datasets.MNIST(PATH, train=True, download=True, \n",
        "                                    transform=transforms.Compose([transforms.ToTensor()]))\n",
        "        val_dataset = datasets.MNIST(PATH, train=False, download=True, \n",
        "                                    transform=transforms.Compose([transforms.ToTensor()]))\n",
        "    elif task == \"cifar10\":\n",
        "        transform_train = [transforms.ToTensor()]\n",
        "        transform_val = [transforms.ToTensor()]\n",
        "\n",
        "        # Perform data augmentation if specified\n",
        "        if augment == \"standard\":\n",
        "            transform_train.extend([transforms.RandomCrop(28), transforms.RandomHorizontalFlip()])\n",
        "            transform_val.extend([transforms.CenterCrop(28)])\n",
        "        elif augment == \"cutmix\":\n",
        "            transform_train.append(v2.CutMix())\n",
        "            transform_val.append(v2.CutMix())\n",
        "\n",
        "        # Add the normalization transform if specified\n",
        "        if transform == \"normalize\":\n",
        "            mean = [0.491, 0.482, 0.447]\n",
        "            std = [0.202, 0.199, 0.201]\n",
        "            transform_train.append(transforms.Normalize(mean, std))\n",
        "            transform_val.append(transforms.Normalize(mean, std))\n",
        "        elif transform == \"pca\":\n",
        "            transform_train.append(Lambda(pca_whitening))\n",
        "            transform_val.append(Lambda(pca_whitening))\n",
        "    \n",
        "        train_dataset = datasets.CIFAR10(PATH, train=True, download=True, \n",
        "                                    transform=transforms.Compose(transform_train))\n",
        "        val_dataset = datasets.CIFAR10(PATH, train=False, download=True, \n",
        "                                transform=transforms.Compose(transform_val))\n",
        "        \n",
        "        # Special processing is required if we want to use MinMaxScaler\n",
        "        if transform == \"minmax\":\n",
        "            # Convert the dataset to NumPy arrays and reshape to 2D for MinMaxScaler\n",
        "            x_train = np.array([data[0].numpy() for data in train_dataset])\n",
        "            x_train = x_train.reshape(x_train.shape[0], -1) # Reshape to 2D\n",
        "            x_val = np.array([data[0].numpy() for data in val_dataset])\n",
        "            x_val = x_val.reshape(x_val.shape[0], -1) # Reshape to 2D\n",
        "\n",
        "            scaler = MinMaxScaler()\n",
        "            \n",
        "            # Apply MinMaxScaler on the reshaped data\n",
        "            train_normalized = scaler.fit_transform(x_train)\n",
        "            val_normalized = scaler.fit_transform(x_val)\n",
        "\n",
        "            # Reshape back to original image shape and convert to tensors\n",
        "            train_normalized = train_normalized.reshape(x_train.shape[0], 3, 32, 32)\n",
        "            train_normalized = torch.from_numpy(train_normalized).float()\n",
        "            val_normalized = val_normalized.reshape(x_val.shape[0], 3, 32, 32)\n",
        "            val_normalized = torch.from_numpy(val_normalized).float()\n",
        "\n",
        "            # Combine normalized data with labels\n",
        "            train_dataset = list(zip(train_normalized, [data[1] for data in train_dataset]))\n",
        "            val_dataset = list(zip(val_normalized, [data[1] for data in val_dataset]))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                        batch_size=batch_size, shuffle=True, pin_memory=cuda, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                        batch_size=batch_size, shuffle=False, pin_memory=cuda, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def epoch(data, model, criterion, optimizer=None, cuda=False, verbose=True):\n",
        "    \"\"\"\n",
        "    Make a pass (called epoch in English) on the data `data` with the\n",
        "     model `model`. Evaluates `criterion` as loss.\n",
        "     If `optimizer` is given, perform a training epoch using\n",
        "     the given optimizer, otherwise, perform an evaluation epoch (no backward)\n",
        "     of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # indicates whether the model is in eval or train mode (some layers behave differently in train and eval)\n",
        "    model.eval() if optimizer is None else model.train()\n",
        "\n",
        "    # objects to store metric averages\n",
        "    avg_loss = AverageMeter()\n",
        "    avg_top1_acc = AverageMeter()\n",
        "    avg_top5_acc = AverageMeter()\n",
        "    avg_batch_time = AverageMeter()\n",
        "    global loss_plot\n",
        "\n",
        "    # we iterate on the batches\n",
        "    tic = time.time()\n",
        "    for i, (input, target) in enumerate(data):\n",
        "\n",
        "        if cuda: # only with GPU, and not with CPU\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        # forward\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # backward if we are training\n",
        "        if optimizer:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # compute metrics\n",
        "        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
        "        batch_time = time.time() - tic\n",
        "        tic = time.time()\n",
        "\n",
        "        # update\n",
        "        avg_loss.update(loss.item())\n",
        "        avg_top1_acc.update(prec1.item())\n",
        "        avg_top5_acc.update(prec5.item())\n",
        "        avg_batch_time.update(batch_time)\n",
        "        # if optimizer and plot_batch_loss:\n",
        "        #     loss_plot.update(avg_loss.val)\n",
        "        # print info\n",
        "        if i % PRINT_INTERVAL == 0:\n",
        "            if verbose:\n",
        "                print('[{0:s} Batch {1:03d}/{2:03d}]\\t'\n",
        "                    'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\\t'\n",
        "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                    'Prec@1 {top1.val:5.1f} ({top1.avg:5.1f})\\t'\n",
        "                    'Prec@5 {top5.val:5.1f} ({top5.avg:5.1f})'.format(\n",
        "                    \"EVAL\" if optimizer is None else \"TRAIN\", i, len(data), batch_time=avg_batch_time, loss=avg_loss,\n",
        "                    top1=avg_top1_acc, top5=avg_top5_acc))\n",
        "            # if optimizer and plot_batch_loss:\n",
        "            #     loss_plot.plot()\n",
        "\n",
        "    # Print summary\n",
        "    if verbose:\n",
        "        print('\\n===============> Total time {batch_time:d}s\\t'\n",
        "            'Avg loss {loss.avg:.4f}\\t'\n",
        "            'Avg Prec@1 {top1.avg:5.2f} %\\t'\n",
        "            'Avg Prec@5 {top5.avg:5.2f} %\\n'.format(\n",
        "            batch_time=int(avg_batch_time.sum), loss=avg_loss,\n",
        "            top1=avg_top1_acc, top5=avg_top5_acc))\n",
        "\n",
        "    return avg_top1_acc, avg_top5_acc, avg_loss\n",
        "\n",
        "def plot_metrics(loss_histories, acc_histories, epochs, plot='same'):\n",
        "    if plot == 'separate':\n",
        "        # Create a 2x2 subplot grid\n",
        "        fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
        "\n",
        "        epochs_range = range(1, epochs + 1)\n",
        "\n",
        "        # Plot Train Accuracy\n",
        "        ax = axs[0, 0]\n",
        "        for params, metrics in acc_histories.items():\n",
        "            ax.plot(epochs_range, metrics['train'], label=params)\n",
        "        ax.set_title('Train Accuracy')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Accuracy (%)')\n",
        "        ax.legend()\n",
        "        ax.grid(False)\n",
        "\n",
        "        # Plot Test Accuracy\n",
        "        ax = axs[0, 1]\n",
        "        for params, metrics in acc_histories.items():\n",
        "            ax.plot(epochs_range, metrics['val'], label=params)\n",
        "        ax.set_title('Test Accuracy')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Accuracy (%)')\n",
        "        ax.legend()\n",
        "        ax.grid(False)\n",
        "\n",
        "        # Plot Train Loss\n",
        "        ax = axs[1, 0]\n",
        "        for params, metrics in loss_histories.items():\n",
        "            ax.plot(epochs_range, metrics['train'], label=params)\n",
        "        ax.set_title('Train Loss')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.legend()\n",
        "        ax.grid(False)\n",
        "\n",
        "        # Plot Test Loss\n",
        "        ax = axs[1, 1]\n",
        "        for params, metrics in loss_histories.items():\n",
        "            ax.plot(epochs_range, metrics['val'], label=params)\n",
        "        ax.set_title('Test Loss')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.legend()\n",
        "        ax.grid(False)\n",
        "    elif plot == 'same':\n",
        "        # Create a 1x2 subplot grid\n",
        "        # Plot train and test accuracy on one plot\n",
        "        # and train and test loss on the other\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "        epochs_range = range(1, epochs + 1)\n",
        "\n",
        "        # Plot Accuracy\n",
        "        ax = axs[0]\n",
        "        for params, metrics in acc_histories.items():\n",
        "            ax.plot(epochs_range, metrics['train'], label=f\"{params} (train)\")\n",
        "            ax.plot(epochs_range, metrics['val'], label=f\"{params} (test)\")\n",
        "        ax.set_title('Accuracy')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Accuracy (%)')\n",
        "        ax.legend()\n",
        "        ax.grid(False)\n",
        "\n",
        "        # Plot Loss\n",
        "        ax = axs[1]\n",
        "        for params, metrics in loss_histories.items():\n",
        "            ax.plot(epochs_range, metrics['train'], label=f\"{params} (train)\")\n",
        "            ax.plot(epochs_range, metrics['val'], label=f\"{params} (test)\")\n",
        "        ax.set_title('Loss')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.legend()\n",
        "        ax.grid(False)\n",
        "    else:\n",
        "        raise ValueError(\"plot should be in ['same', 'separate']\")\n",
        "\n",
        "    # Adjust layout and display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main(bs_list=[128], \n",
        "         lr_list=[0.1], \n",
        "         epochs=5, \n",
        "         cuda=False, \n",
        "         task=\"mnist\", \n",
        "         verbose=True, \n",
        "         transform=None, \n",
        "         plot='same', \n",
        "         augment=None,\n",
        "         scheduler=False,\n",
        "         model='ConvNet',\n",
        "         prob=0.5,\n",
        "         optimizer=\"SGD\"):\n",
        "    \n",
        "    # Initialize dictionaries to store loss and accuracy histories\n",
        "    loss_histories = {}\n",
        "    acc_histories = {}\n",
        "    \n",
        "    for batch_size in bs_list:\n",
        "        # Get the data once outside the loop\n",
        "        train_loader, test_loader = get_dataset(batch_size, cuda, task, transform=transform, augment=augment)\n",
        "        for lr in lr_list:\n",
        "            print(f\"\\nTraining with lr={lr} and batch_size={batch_size}\\n\")\n",
        "            \n",
        "            # Define model, loss, optimizer for each learning rate\n",
        "            if model == \"ConvNet\":\n",
        "                model = ConvNet()\n",
        "            elif model == \"ConvNet_v2\":\n",
        "                model = ConvNet_v2()\n",
        "            elif model == \"ConvNet_v3\":\n",
        "                model = ConvNet_v3(prob=prob)\n",
        "            elif model == \"ConvNet_v4\":\n",
        "                model = ConvNet_v4()\n",
        "            elif model == \"ConvNet_v5\":\n",
        "                model = ConvNet_v5(prob=prob)\n",
        "            else:\n",
        "                raise ValueError(\"model should be in ['ConvNet', 'ConvNet_v2', 'ConvNet_v3', 'ConvNet_v4', 'ConvNet_v5']\")\n",
        "            \n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            if optimizer == \"Adam\":\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "            elif optimizer == \"SGD\":\n",
        "                optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "            else:\n",
        "                raise ValueError(\"optimizer should be in ['Adam', 'SGD']\")\n",
        "\n",
        "            if scheduler:\n",
        "                lr_sched = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "            if cuda:\n",
        "                cudnn.benchmark = True\n",
        "                model = model.cuda()\n",
        "                criterion = criterion.cuda()\n",
        "            \n",
        "            # Initialize lists to store metrics per epoch\n",
        "            train_losses = []\n",
        "            val_losses = []\n",
        "            train_accuracies = []\n",
        "            val_accuracies = []\n",
        "            \n",
        "            # We iterate over epochs\n",
        "            for epoch_num in range(epochs):\n",
        "                if verbose:\n",
        "                    print(f\"=== Epoch {epoch_num+1}/{epochs} ===\")\n",
        "                \n",
        "                # Train phase\n",
        "                top1_acc, top5_acc, train_loss = epoch(train_loader, model, criterion, optimizer, cuda, verbose)\n",
        "                train_losses.append(train_loss.avg)\n",
        "                train_accuracies.append(top1_acc.avg)\n",
        "                \n",
        "                # Validation phase\n",
        "                val_top1_acc, val_top5_acc, val_loss = epoch(test_loader, model, criterion, cuda=cuda, verbose=verbose)\n",
        "                val_losses.append(val_loss.avg)\n",
        "                val_accuracies.append(val_top1_acc.avg)\n",
        "\n",
        "                if scheduler:\n",
        "                    lr_sched.step()\n",
        "            \n",
        "            # Store the metrics history for these parameters\n",
        "            params = f\"lr={lr} bs={batch_size}\"\n",
        "            loss_histories[params] = {'train': train_losses, 'val': val_losses}\n",
        "            acc_histories[params] = {'train': train_accuracies, 'val': val_accuracies}\n",
        "    \n",
        "    # After all learning rates have been processed, plot all metrics\n",
        "    plot_metrics(loss_histories, acc_histories, epochs, plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instructions\n",
        "\n",
        "- **bs_list** (`list`, default `[128]`): A list of batch sizes to use during training. The function will iterate over each batch size in the list.\n",
        "\n",
        "- **lr_list** (`list`, default `[0.1]`): A list of learning rates to use for the optimizer. The function will train the model separately for each learning rate.\n",
        "\n",
        "- **epochs** (`int`, default `5`): The number of epochs to train the model for each combination of batch size and learning rate.\n",
        "\n",
        "- **cuda** (`bool`, default `False`): Whether to use CUDA (GPU acceleration) for training. Set to `True` if you have a compatible GPU and CUDA installed.\n",
        "\n",
        "- **task** (`str`, default `\"mnist\"`): The dataset to use. Options are:\n",
        "  - `\"mnist\"`: Use the MNIST dataset.\n",
        "  - `\"cifar10\"`: Use the CIFAR-10 dataset.\n",
        "\n",
        "- **verbose** (`bool`, default `True`): Whether to print detailed training progress and metrics.\n",
        "\n",
        "- **transform** (`str` or `None`, default `None`): Data transformations to apply. Options are:\n",
        "  - `\"normalize\"`: Apply normalization to the dataset.\n",
        "  - `\"pca\"`\n",
        "  - `\"minmax\"`\n",
        "  - `None`: No additional transformations.\n",
        "\n",
        "- **plot** (`str`, default `'same'`): How to plot the training metrics after training. Options are:\n",
        "  - `'same'`: Plot training and validation accuracy and loss on the same graphs.\n",
        "  - `'separate'`: Plot training and validation metrics on separate graphs.\n",
        "\n",
        "- **augment** (`str` or `None`, default `None`): Whether to apply data augmentation techniques to the training data (only applicable when `task=\"cifar10\"`).\n",
        "  - `\"standard\"`: Apply the basic augmentation technique discussed in the report (crops and random horizontal flips).\n",
        "  - `\"cutmix\"`: Apply CutMix transform to the dataset.\n",
        "\n",
        "- **scheduler** (`bool`, default `False`): Whether to use a learning rate scheduler during training.\n",
        "\n",
        "- **model** (`str`, default `'ConvNet'`): The model architecture to use. Options are:\n",
        "  - `'ConvNet'`: The base convolutional neural network.\n",
        "  - `'ConvNet_v2'`: A variant with increased capacity.\n",
        "  - `'ConvNet_v3'`: Adds dropout layers.\n",
        "  - `'ConvNet_v4'`: Adds batch normalization layers.\n",
        "  - `'ConvNet_v5'`: Combines batch normalization and dropout.\n",
        "\n",
        "- **prob** (`float`, default `0.5`): Dropout probability. Only used if the selected model includes dropout layers (e.g., `'ConvNet_v3'` or `'ConvNet_v5'`).\n",
        "\n",
        "-**optimizer** (`str`, default `'SGD'`): Selects the optimizer.\n",
        "  - `'SGD'`: Stochastic Gradient Descent.\n",
        "  - `'Adam'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXoiJsO0PI5C"
      },
      "outputs": [],
      "source": [
        "lrs = [0.1]\n",
        "bs_list = [128]\n",
        "main(bs_list=bs_list, lr_list=lrs, cuda=True, epochs=60, task=\"cifar10\", \n",
        "     model=\"ConvNet_v2\", plot='same', transform='pca', augment=None, scheduler=False, optimizer=\"SGD\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "general",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
